{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Loading an image from kaggle directly in Google Colab**"
      ],
      "metadata": {
        "id": "nHSD5C6CXdbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!pip install opendataset\n",
        "import os\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/root/.kaggle\"\n",
        "!cp /content/kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "!unzip -q chest-xray-pneumonia.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2GEZZo8cRdE",
        "outputId": "8b6377cb-93ae-449a-ce16-7b412f0ef95b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opendataset (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opendataset\u001b[0m\u001b[31m\n",
            "\u001b[0mDataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "License(s): other\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VGG-16 Implementation**"
      ],
      "metadata": {
        "id": "AaXEbYUpZFiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary Libraries**"
      ],
      "metadata": {
        "id": "6ZceMkZpbjK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  import tensorflow for building and training the model.\n",
        "# import ImageDataGenerator for data augmentation and preprocessing.\n",
        "# import VGG16 to load the pre-trained VGG-16 model.\n",
        "# import Dense and Flatten layers for building the custom classification head.\n",
        "# import Model to create the final model.#"
      ],
      "metadata": {
        "id": "jQIlxXlubAjL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "odO_twaFXvl4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing and augmentation**"
      ],
      "metadata": {
        "id": "-yHvXIdab09Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'chest_xray/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    'chest_xray/val',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'chest_xray/test',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uqkskIAY8H4",
        "outputId": "5dd9a162-1ce2-4605-d72e-072da598fb5a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create two ImageDataGenerator instances, one for training data with augmentation and one for validation and testing data without augmentation.\n",
        "\n",
        " Use flow_from_directory to load images from the respective directories and generate batches of data for training, validation, and testing.\n",
        "\n",
        "Specify target_size to resize images to (224, 224) which is the input size for VGG-16.\n",
        "\n",
        " Set class_mode to 'binary' as this is a binary classification problem (pneumonia or normal)."
      ],
      "metadata": {
        "id": "kSM-0jevc53j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Build and compile the model:**"
      ],
      "metadata": {
        "id": "sr8M9NH6dIoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4trPp8SdGDL",
        "outputId": "d1cb4861-bedd-4d63-f9ec-af84ea2d528e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load the pre-trained VGG-16 model with\n",
        "weights from ImageNet and exclude the top classification layers (include_top=False).\n",
        "\n",
        "Add a custom classification head on top of the base model using Flatten, Dense, and activation functions.\n",
        "\n",
        "Freeze the layers of the base model to prevent them from being updated during training (layer.trainable = False).\n",
        "\n",
        "Compile the model using the Adam optimizer, binary cross-entropy loss function, and accuracy as the evaluation metric."
      ],
      "metadata": {
        "id": "lMBk42dCdp4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Train the model:**"
      ],
      "metadata": {
        "id": "VKQX7hy0ec6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl1dojc7Y8MF",
        "outputId": "89a3e602-b587-448c-8c0a-f9fc9c30d9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3324s\u001b[0m 20s/step - accuracy: 0.8472 - loss: 0.6073 - val_accuracy: 0.7500 - val_loss: 0.3326\n",
            "Epoch 2/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3270s\u001b[0m 20s/step - accuracy: 0.9612 - loss: 0.1073 - val_accuracy: 0.6875 - val_loss: 0.6635\n",
            "Epoch 3/10\n",
            "\u001b[1m 79/163\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m27:51\u001b[0m 20s/step - accuracy: 0.9567 - loss: 0.0995"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model using the fit method, providing the training and validation data generators.\n",
        "\n",
        "Specify the number of epochs and steps per epoch.\n",
        "\n",
        "To see the output, run the code."
      ],
      "metadata": {
        "id": "bqiOyCbPemy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Evaluate the model:**"
      ],
      "metadata": {
        "id": "knwxy8BKeuea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "sTw3rtCvetMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "NEjy_YN0k5JY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Prep1pWFk4Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RkoDCMFGk4Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oBISSTX5k4Yi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}